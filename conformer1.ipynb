{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-28 14:54:04--  https://www.openslr.org/resources/12/train-clean-100.tar.gz\n",
      "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
      "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://openslr.elda.org/resources/12/train-clean-100.tar.gz [following]\n",
      "--2024-11-28 14:54:05--  https://openslr.elda.org/resources/12/train-clean-100.tar.gz\n",
      "Resolving openslr.elda.org (openslr.elda.org)... 141.94.109.138, 2001:41d0:203:ad8a::\n",
      "Connecting to openslr.elda.org (openslr.elda.org)|141.94.109.138|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6387309499 (5.9G) [application/x-gzip]\n",
      "Saving to: ‘./train-clean-100.tar.gz’\n",
      "\n",
      "train-clean-100.tar 100%[===================>]   5.95G  28.8MB/s    in 3m 35s  \n",
      "\n",
      "2024-11-28 14:57:40 (28.4 MB/s) - ‘./train-clean-100.tar.gz’ saved [6387309499/6387309499]\n",
      "\n",
      "--2024-11-28 14:58:01--  https://www.openslr.org/resources/12/dev-clean.tar.gz\n",
      "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
      "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://openslr.elda.org/resources/12/dev-clean.tar.gz [following]\n",
      "--2024-11-28 14:58:01--  https://openslr.elda.org/resources/12/dev-clean.tar.gz\n",
      "Resolving openslr.elda.org (openslr.elda.org)... 141.94.109.138, 2001:41d0:203:ad8a::\n",
      "Connecting to openslr.elda.org (openslr.elda.org)|141.94.109.138|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 337926286 (322M) [application/x-gzip]\n",
      "Saving to: ‘./dev-clean.tar.gz’\n",
      "\n",
      "dev-clean.tar.gz    100%[===================>] 322.27M  29.3MB/s    in 12s     \n",
      "\n",
      "2024-11-28 14:58:13 (27.5 MB/s) - ‘./dev-clean.tar.gz’ saved [337926286/337926286]\n",
      "\n",
      "--2024-11-28 14:58:15--  https://www.openslr.org/resources/12/test-clean.tar.gz\n",
      "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
      "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://openslr.elda.org/resources/12/test-clean.tar.gz [following]\n",
      "--2024-11-28 14:58:16--  https://openslr.elda.org/resources/12/test-clean.tar.gz\n",
      "Resolving openslr.elda.org (openslr.elda.org)... 141.94.109.138, 2001:41d0:203:ad8a::\n",
      "Connecting to openslr.elda.org (openslr.elda.org)|141.94.109.138|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 346663984 (331M) [application/x-gzip]\n",
      "Saving to: ‘./test-clean.tar.gz’\n",
      "\n",
      "test-clean.tar.gz   100%[===================>] 330.60M  27.6MB/s    in 13s     \n",
      "\n",
      "2024-11-28 14:58:29 (25.8 MB/s) - ‘./test-clean.tar.gz’ saved [346663984/346663984]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.openslr.org/resources/12/train-clean-100.tar.gz -P ./\n",
    "!tar -xf train-clean-100.tar.gz -C ./\n",
    "\n",
    "!wget https://www.openslr.org/resources/12/dev-clean.tar.gz -P ./\n",
    "!tar -xf dev-clean.tar.gz -C ./\n",
    "\n",
    "!wget https://www.openslr.org/resources/12/test-clean.tar.gz -P ./\n",
    "!tar -xf test-clean.tar.gz -C ./\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the base dataset directory\n",
    "base_dir = \"/kaggle/working/LibriSpeech\"\n",
    "\n",
    "# Define specific dataset directories\n",
    "train_dir = os.path.join(base_dir, \"train-clean-100\")\n",
    "dev_dir = os.path.join(base_dir, \"dev-clean\")\n",
    "test_dir = os.path.join(base_dir, \"test-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960_asr_ls960.pth\" to /home/nrelab-titan/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960_asr_ls960.pth\n",
      "100%|██████████| 360M/360M [00:05<00:00, 66.9MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train dataset...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/working/LibriSpeech/train-clean-100'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 224\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Process train dataset\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing train dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 224\u001b[0m df_train \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Process validation dataset (dev-clean)\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing validation dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 179\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m    177\u001b[0m flac_files \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    178\u001b[0m labels_files \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    180\u001b[0m     speaker_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;28mdir\u001b[39m)\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(speaker_dir):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/LibriSpeech/train-clean-100'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import csv\n",
    "\n",
    "# Conformer Block\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, ff_dim, kernel_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ff1 = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.attention = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.conv_module = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_model, kernel_size, padding=kernel_size // 2, groups=d_model),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=1),\n",
    "            nn.BatchNorm1d(d_model)\n",
    "        )\n",
    "        self.ff2 = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        # First feedforward\n",
    "        x = x + 0.5 * self.ff1(x)\n",
    "\n",
    "        # Multi-head attention\n",
    "        attn_output, _ = self.attention(x, x, x, key_padding_mask=src_key_padding_mask)\n",
    "        x = x + attn_output\n",
    "\n",
    "        # Convolutional module\n",
    "        x_conv = x.permute(0, 2, 1)  # Convert to (B, C, T) for Conv1d\n",
    "        x_conv = self.conv_module(x_conv)\n",
    "        x_conv = x_conv.permute(0, 2, 1)  # Back to (B, T, C)\n",
    "        x = x + x_conv\n",
    "\n",
    "        # Second feedforward\n",
    "        x = x + 0.5 * self.ff2(x)\n",
    "\n",
    "        # Final normalization\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "# Simple Conformer Model\n",
    "class SimpleConformer(nn.Module):\n",
    "    def __init__(self, nheads=1, d_model=768, num_layers=1, ff_dim=2048, kernel_size=15, dropout=0.1):\n",
    "        super().__init__()\n",
    "        bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "        self.sr = bundle.sample_rate\n",
    "        self.labels = bundle.get_labels()\n",
    "        self.num_chars = len(self.labels)\n",
    "        self.ƒ = bundle.get_model().extract_features  # Only the feature extractor\n",
    "\n",
    "        # Project input features to d_model for Conformer\n",
    "        self.linear = nn.Linear(768, d_model)\n",
    "        self.conformers = nn.ModuleList(\n",
    "            [ConformerBlock(d_model, nheads, ff_dim, kernel_size, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # Final output layer to predict characters\n",
    "        self.char_out = nn.Linear(d_model, self.num_chars)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = self.create_mask(x)\n",
    "        x = self.linear(x)  # Project to d_model dimension\n",
    "        for conformer in self.conformers:\n",
    "            x = conformer(x, src_key_padding_mask=mask)\n",
    "        x = self.char_out(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def create_mask(self, x):\n",
    "        # Create mask for padded positions (where first feature dim is zero)\n",
    "        return (x[:, :, 0] == PAD_IDX)  # True where padding is applied \n",
    "\n",
    "PAD_IDX = 0  # Use zero for padding\n",
    "\n",
    "# Data Augmentation Functions\n",
    "def add_noise(waveform, noise_level=0.005):\n",
    "    noise = torch.randn_like(waveform)\n",
    "    augmented = waveform + noise_level * noise\n",
    "    augmented = augmented.clamp(-1.0, 1.0)\n",
    "    return augmented\n",
    "\n",
    "def random_gain(waveform, min_gain=0.8, max_gain=1.2):\n",
    "    gain = random.uniform(min_gain, max_gain)\n",
    "    return waveform * gain\n",
    "\n",
    "def time_stretch(waveform, rate=1.0):\n",
    "    if rate == 1.0:\n",
    "        return waveform\n",
    "    stretched_waveform = torchaudio.transforms.Resample(orig_freq=decoder.sr, new_freq=int(decoder.sr * rate))(waveform)\n",
    "    return stretched_waveform\n",
    "\n",
    "def pitch_shift(waveform, sample_rate, n_steps=0):\n",
    "    return torchaudio.transforms.PitchShift(sample_rate=sample_rate, n_steps=n_steps)(waveform)\n",
    "\n",
    "# Custom Dataset with Data Augmentation\n",
    "class DataSet(Dataset):\n",
    "    def __init__(self, local_df, encoder, decoder, augment=False):\n",
    "        self.df = local_df\n",
    "        self.sr = decoder.sr\n",
    "        self.decoder = decoder\n",
    "        self.encoder = encoder\n",
    "        self.augment = augment\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        dir = self.df.iloc[index].dir\n",
    "        ids = self.df.iloc[index].ids\n",
    "        labels = self.df.iloc[index].labels\n",
    "        labels = labels.replace(\" \", \"|\")  # Mark spaces with \"|\"\n",
    "\n",
    "        # Convert characters to indices\n",
    "        char_to_idx = {char: idx for idx, char in enumerate(self.decoder.labels)}\n",
    "        ground_truth_token = torch.tensor([char_to_idx[c] for c in labels if c in char_to_idx])\n",
    "\n",
    "        # Load audio and extract features\n",
    "        waveform, sample_rate = torchaudio.load(dir)\n",
    "        if sample_rate != self.decoder.sr:\n",
    "            waveform = torchaudio.functional.resample(waveform, sample_rate, self.decoder.sr)\n",
    "\n",
    "        if self.augment:\n",
    "            waveform = self.apply_augmentation(waveform)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            latent, _ = self.encoder(waveform)\n",
    "\n",
    "        latent = latent[-1].squeeze()\n",
    "        return latent, ground_truth_token\n",
    "\n",
    "    def apply_augmentation(self, waveform):\n",
    "        # Apply random augmentations\n",
    "        if random.random() < 0.5:\n",
    "            waveform = add_noise(waveform)\n",
    "        if random.random() < 0.5:\n",
    "            waveform = random_gain(waveform)\n",
    "        if random.random() < 0.5:\n",
    "            rate = random.uniform(0.9, 1.1)\n",
    "            waveform = time_stretch(waveform, rate)\n",
    "        if random.random() < 0.5:\n",
    "            n_steps = random.randint(-2, 2)\n",
    "            waveform = pitch_shift(waveform, self.sr, n_steps)\n",
    "        return waveform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "# Collate function to handle padding in batches\n",
    "def collate_fn(batch):\n",
    "    latents, labels = zip(*batch)\n",
    "    latents_padded = pad_sequence(latents, batch_first=True, padding_value=PAD_IDX)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=PAD_IDX)\n",
    "    return latents_padded, labels_padded\n",
    "\n",
    "# Function to process dataset directories\n",
    "def process_dataset(data_dir):\n",
    "    flac_files = []\n",
    "    labels_files = []\n",
    "    for dir in os.listdir(data_dir):\n",
    "        speaker_dir = os.path.join(data_dir, dir)\n",
    "        if os.path.isdir(speaker_dir):\n",
    "            for chapter in os.listdir(speaker_dir):\n",
    "                chapter_dir = os.path.join(speaker_dir, chapter)\n",
    "                if os.path.isdir(chapter_dir):\n",
    "                    for file in os.listdir(chapter_dir):\n",
    "                        if file.endswith(\".flac\"):\n",
    "                            flac_files.append(os.path.join(chapter_dir, file))\n",
    "                        if file.endswith(\".txt\"):\n",
    "                            labels_files.append(os.path.join(chapter_dir, file))\n",
    "    # Process labels and create dataframe\n",
    "    dirs = []\n",
    "    all_labels = []\n",
    "    for labels in labels_files:\n",
    "        with open(labels, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "        lines = [line.strip(\"\\n\") for line in lines]\n",
    "        for _ in range(len(lines)):\n",
    "            dirs.append(labels)\n",
    "        all_labels.extend(lines)\n",
    "    ids = [x.split(\" \")[0] for x in all_labels]\n",
    "    true_labels = [\" \".join(x.split(\" \")[1:]) for x in all_labels]\n",
    "    flac_dict = {flac.split(\"/\")[-1].split(\".flac\")[0]: flac for flac in flac_files}\n",
    "    organized_flac = []\n",
    "    for id in tqdm(ids):\n",
    "        if id in flac_dict:\n",
    "            organized_flac.append(flac_dict[id])\n",
    "        else:\n",
    "            print(f\"ERROR: {id} not found in flac files.\")\n",
    "    df = pd.DataFrame()\n",
    "    df[\"dir\"] = organized_flac\n",
    "    df[\"ids\"] = ids\n",
    "    df[\"labels\"] = true_labels\n",
    "    return df\n",
    "\n",
    "# Initialize model to access sample rate for augmentation transforms\n",
    "# This is necessary because some transforms like pitch_shift require sample rate\n",
    "decoder_init = SimpleConformer(nheads=4, d_model=768, num_layers=6, ff_dim=2048, kernel_size=31, dropout=0.1)\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "decoder_init.sr = bundle.sample_rate\n",
    "\n",
    "\n",
    "# Process train dataset\n",
    "print(\"Processing train dataset...\")\n",
    "df_train = process_dataset(train_dir)\n",
    "\n",
    "# Process validation dataset (dev-clean)\n",
    "print(\"Processing validation dataset...\")\n",
    "df_valid = process_dataset(dev_dir)\n",
    "\n",
    "# Process test dataset (test-clean)\n",
    "print(\"Processing test dataset...\")\n",
    "df_test = process_dataset(test_dir)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "decoder = SimpleConformer(nheads=2, d_model=384, num_layers=3, ff_dim=1024, kernel_size=31, dropout=0.1)\n",
    "\n",
    "# Create datasets and dataloaders with augmentation for training set\n",
    "train_ds = DataSet(df_train, decoder_init.ƒ, decoder, augment=True)\n",
    "valid_ds = DataSet(df_valid, decoder_init.ƒ, decoder, augment=False)\n",
    "test_ds = DataSet(df_test, decoder_init.ƒ, decoder, augment=False)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "test_dl = DataLoader(test_ds, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Optimizer and Loss function\n",
    "optim = torch.optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=False)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Path to latest checkpoint\n",
    "checkpoint_path = 'latest_model_checkpoint.pth'\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    decoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    valid_losses = checkpoint['valid_losses']\n",
    "else:\n",
    "    print(\"No checkpoint found, starting training from scratch.\")\n",
    "    start_epoch = 0\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "# Training Loop with Prediction Output\n",
    "decoder.to(device)\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    # Training Phase\n",
    "    decoder.train()\n",
    "    train_running_loss = 0\n",
    "\n",
    "    # Open CSV file for training predictions\n",
    "    with open(f'train_predictions_epoch_{epoch+1}.csv', 'w', newline='', encoding='utf-8') as train_csvfile:\n",
    "        train_csv_writer = csv.writer(train_csvfile)\n",
    "        train_csv_writer.writerow(['Prediction', 'Ground Truth'])\n",
    "\n",
    "        for latents, labels in tqdm(train_dl):\n",
    "            optim.zero_grad()\n",
    "            latents, labels = latents.to(device), labels.to(device)\n",
    "            prediction = decoder(latents)\n",
    "            prediction = prediction.permute(1, 0, 2)  # (T, N, C)\n",
    "\n",
    "            # Compute input and target lengths\n",
    "            input_lengths = torch.tensor([l[l[:, 0] != PAD_IDX].shape[0] for l in latents]).to(device)\n",
    "            target_lengths = torch.tensor([len(lbl[lbl != PAD_IDX]) for lbl in labels], dtype=torch.long).to(device)\n",
    "\n",
    "            loss = criterion(prediction, labels, input_lengths, target_lengths)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            train_running_loss += loss.item()\n",
    "\n",
    "            # Decode the predictions and ground truth\n",
    "            max_indices = torch.argmax(prediction, dim=2)  # Get predicted characters as indices\n",
    "            predicted_str = \"\".join([decoder.labels[idx] for idx in max_indices[:, 0].cpu().numpy() if idx != PAD_IDX])\n",
    "            predicted_str = predicted_str.replace(\"-\", \"\").replace(\"|\", \" \")  # Clean up padding and special tokens\n",
    "\n",
    "            # Decode the ground truth for comparison\n",
    "            gt_str = \"\".join([decoder.labels[idx.item()] for idx in labels[0] if idx != PAD_IDX])\n",
    "            gt_str = gt_str.replace(\"-\", \"\").replace(\"|\", \" \")\n",
    "\n",
    "            print(f\"Batch Prediction: {predicted_str}\")\n",
    "            print(f\"Batch Ground Truth: {gt_str}\")\n",
    "            print(\"\")\n",
    "            train_csv_writer.writerow([predicted_str, gt_str])\n",
    "\n",
    "    train_losses.append(train_running_loss / len(train_dl))\n",
    "\n",
    "    # Validation Phase\n",
    "    decoder.eval()\n",
    "    valid_running_loss = 0\n",
    "\n",
    "    # Open CSV file for validation predictions\n",
    "    with open(f'valid_predictions_epoch_{epoch+1}.csv', 'w', newline='', encoding='utf-8') as valid_csvfile:\n",
    "        valid_csv_writer = csv.writer(valid_csvfile)\n",
    "        valid_csv_writer.writerow(['Prediction', 'Ground Truth'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for latents, labels in tqdm(valid_dl):\n",
    "                latents, labels = latents.to(device), labels.to(device)\n",
    "                prediction = decoder(latents)\n",
    "                prediction = prediction.permute(1, 0, 2)\n",
    "\n",
    "                input_lengths = torch.tensor([l[l[:, 0] != PAD_IDX].shape[0] for l in latents]).to(device)\n",
    "                target_lengths = torch.tensor([len(lbl[lbl != PAD_IDX]) for lbl in labels], dtype=torch.long).to(device)\n",
    "\n",
    "                loss = criterion(prediction, labels, input_lengths, target_lengths)\n",
    "                valid_running_loss += loss.item()\n",
    "\n",
    "                # Decode the predictions and ground truth\n",
    "                max_indices = torch.argmax(prediction, dim=2)  # (T, N)max_indices = torch.argmax(prediction, dim=2)  # Get predicted characters as indices\n",
    "        predicted_str = \"\".join([decoder.labels[idx] for idx in max_indices[:, 0].cpu().numpy() if idx != PAD_IDX])\n",
    "        predicted_str = predicted_str.replace(\"-\", \"\").replace(\"|\", \" \")  # Clean up padding and special tokens\n",
    "\n",
    "        # Decode the ground truth for comparison\n",
    "        gt_str = \"\".join([decoder.labels[idx.item()] for idx in labels[0] if idx != PAD_IDX])\n",
    "        gt_str = gt_str.replace(\"-\", \"\").replace(\"|\", \" \")\n",
    "\n",
    "        print(f\"Batch Prediction: {predicted_str}\")\n",
    "        print(f\"Batch Ground Truth: {gt_str}\")\n",
    "        print(\"\")\n",
    "        valid_csv_writer.writerow([predicted_str, gt_str])\n",
    "\n",
    "    valid_losses.append(valid_running_loss / len(valid_dl))\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary - Train Loss: {train_losses[-1]:.4f}, Valid Loss: {valid_losses[-1]:.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state_dict\": decoder.state_dict(),\n",
    "        \"train_losses\": train_losses,\n",
    "        \"valid_losses\": valid_losses\n",
    "    }\n",
    "    torch.save(checkpoint, f\"{epoch+1}_model_checkpoint.pth\")\n",
    "    torch.save(checkpoint, checkpoint_path)  # Save latest checkpoint\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform CTC decoding\n",
    "def ctc_decode(predictions, blank_index=0):\n",
    "    # predictions: list or numpy array of label indices\n",
    "    output = []\n",
    "    prev_label = None\n",
    "    for l in predictions:\n",
    "        if l != blank_index and l != prev_label:\n",
    "            output.append(l)\n",
    "        prev_label = l\n",
    "    return output\n",
    "\n",
    "# Function to compute edit distance\n",
    "def edit_distance(seq1, seq2):\n",
    "    # Initialize the matrix\n",
    "    m = len(seq1)\n",
    "    n = len(seq2)\n",
    "    D = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m+1):\n",
    "        D[i][0] = i\n",
    "    for j in range(n+1):\n",
    "        D[0][j] = j\n",
    "    # Compute the edit distance\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            if seq1[i-1] == seq2[j-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            D[i][j] = min(D[i-1][j] +1,      # Deletion\n",
    "                          D[i][j-1] +1,      # Insertion\n",
    "                          D[i-1][j-1] + cost)  # Substitution\n",
    "    return D[m][n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "# Evaluate on Test Set\n",
    "print(\"Evaluating on test dataset...\")\n",
    "decoder.eval()\n",
    "test_running_loss = 0\n",
    "total_distance = 0\n",
    "total_words = 0\n",
    "\n",
    "manual_pred_sentences = []\n",
    "manual_gt_sentences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for latents, labels in tqdm(test_dl):\n",
    "        latents, labels = latents.to(device), labels.to(device)\n",
    "        prediction = decoder(latents)\n",
    "        prediction = prediction.permute(1, 0, 2)\n",
    "\n",
    "        input_lengths = torch.tensor([l[l[:, 0] != PAD_IDX].shape[0] for l in latents]).to(device)\n",
    "        target_lengths = torch.tensor([len(lbl[lbl != PAD_IDX]) for lbl in labels], dtype=torch.long).to(device)\n",
    "\n",
    "        loss = criterion(prediction, labels, input_lengths, target_lengths)\n",
    "        test_running_loss += loss.item()\n",
    "\n",
    "        # Decode predictions and ground truths\n",
    "        max_indices = torch.argmax(prediction, dim=2)\n",
    "        for i in range(latents.size(0)):\n",
    "            predicted_indices = max_indices[:, i].cpu().numpy()\n",
    "            predicted_indices = ctc_decode(predicted_indices, blank_index=0)\n",
    "            predicted_str = \"\".join([decoder.labels[idx] for idx in predicted_indices])\n",
    "            predicted_str = predicted_str.replace(\"-\", \"\").replace(\"|\", \" \").strip()\n",
    "\n",
    "            gt_indices = labels[i]\n",
    "            gt_str = \"\".join([decoder.labels[idx.item()] for idx in gt_indices if idx != PAD_IDX])\n",
    "            gt_str = gt_str.replace(\"-\", \"\").replace(\"|\", \" \").strip()\n",
    "\n",
    "            # Collect sentences for `jiwer` calculation\n",
    "            manual_pred_sentences.append(predicted_str)\n",
    "            manual_gt_sentences.append(gt_str)\n",
    "\n",
    "            # Compute manual WER\n",
    "            pred_words = predicted_str.strip().split()\n",
    "            gt_words = gt_str.strip().split()\n",
    "            distance = edit_distance(pred_words, gt_words)\n",
    "            total_distance += distance\n",
    "            total_words += len(gt_words)\n",
    "\n",
    "# Calculate manual WER\n",
    "manual_wer_score = total_distance / total_words\n",
    "print(f\"Test Loss: {test_running_loss / len(test_dl):.4f}\")\n",
    "print(f\"Manual WER: {manual_wer_score:.4f}\")\n",
    "\n",
    "# Calculate WER using jiwer\n",
    "jiwer_wer_score = wer(manual_gt_sentences, manual_pred_sentences)\n",
    "print(f\"jiwer WER: {jiwer_wer_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
